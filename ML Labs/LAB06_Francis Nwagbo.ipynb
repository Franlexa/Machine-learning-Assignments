{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 06 : Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full Name** : [ Francis Nwagbo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Type, Dict, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.Implementation of the Q-learning algorithm\n",
    "\n",
    "- In this part, we will implement a reinforcement learning algorithm.\n",
    "- Two classes will be implemented: \n",
    "    1. The agent \n",
    "    2. The environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### I.1. Agent\n",
    "\n",
    "Here, we start by implementing the agent's functions.\n",
    "\n",
    "#### I.1.1. Creation of the Q table\n",
    "\n",
    "Given $n$ states and $m$ actions, we create a matrix $Q[n, m]$ initialized with $0$s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the function to create the table Q\n",
    "\n",
    "def create_Q(nb_states: int, nb_actions: int) -> np.ndarray:\n",
    "    # hint : use numpy function to generate the table\n",
    "    # Return a new array of given shape, filled with zeros. \n",
    "    \n",
    "    ### CODE 01 ###\n",
    "    ### BEGIN : Q_table = np.zeros((nb_states, nb_actions))\n",
    "    \n",
    "    Q_table = None\n",
    "    \n",
    "    ### END\n",
    "    \n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# array([[0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Q5_3 = create_Q(5, 3)\n",
    "\n",
    "Q5_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.2. Exploration & Exploitation of the Q table\n",
    "- In both functions, we choose an integer between $0$ and $m$ (number of actions).\n",
    "- In the exploration function, we choose this number randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the exploration function\n",
    "\n",
    "def exploration(Q: np.ndarray) -> int:\n",
    "    # hint :  Use np.random.randint() \n",
    "    \n",
    "    ### CODE 02 ###\n",
    "    ### BEGIN :  nb_actions = Q.shape[1]\n",
    "    # action_to_explore = np.random.randint(nb_actions)   \n",
    "    \n",
    "    nb_actions = None\n",
    "    action_to_explore = None\n",
    "    \n",
    "    ### END    \n",
    "    \n",
    "    return action_to_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# a random number in {0, 1, 2}\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "exploration(Q5_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the exploitation, the action with the maximum value is chosen from those of the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the exploitation function\n",
    "\n",
    "def exploitation(Q: np.ndarray, state: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns the indice of the best action for the selected stat, the action with the maximum values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : array_like, Input array, The Q table/matrix.\n",
    "    state : indice of the stat.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    index_best_action : int\n",
    "    \n",
    "    \"\"\"\n",
    "    # Hint : Remmeber that Rows and Columns represent respectively the possible \"Stats\" and \"Actions\" of the agent.\n",
    "    # Return the indice not the maximaum value\n",
    "\n",
    "    ### CODE 03 ###\n",
    "    ### BEGIN : state_values = Q[state, :]\n",
    "    # Find the index of the action with the maximum Q-value\n",
    "    index_best_action = np.argmax(state_values)\n",
    "    \n",
    "    index_best_action = None\n",
    "    \n",
    "    ### END\n",
    "    return index_best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (2, 0, 1, 2, 1)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Q_t = np.array([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [1.0, 0.5, 0.7],\n",
    "    [0.5, 1.0, 0.8],\n",
    "    [0.2, 0.8, 0.9],\n",
    "    [0.2, 1.0, 0.3]\n",
    "])\n",
    "\n",
    "exploitation(Q_t, 0), exploitation(Q_t, 1), exploitation(Q_t, 2), exploitation(Q_t, 3), exploitation(Q_t, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(Q: np.ndarray, state: int, epsilon: float=0.2) -> int:\n",
    "    if np.random.random() < epsilon:\n",
    "        return exploration(Q)\n",
    "    else:\n",
    "        return exploitation(Q, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# Either 2 or a random number in {0, 1, 2}\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "choose_action(Q_t, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.3. Update of the table Q\n",
    "\n",
    "This equation is how our Q-values are updated over time :\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha * (r + \\gamma * \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n",
    "$$\n",
    "\n",
    "With :\n",
    "- $Q(s_t, a_t)$ (```Q```): the entry $[s_t,a_t]$ of the $Q$ table.\n",
    "- $\\alpha $ : Is the learning rate (```alpha```).\n",
    "- $ s_t $ : The current state (```state```).\n",
    "- $ a_t $ : the choosed action to take `(```action```).\n",
    "- $ r $ : Immediate reward (```r```). \n",
    "- $ \\gamma $ : Discount-factor (```gamma```).\n",
    "- $ Q(s_{t+1}, a) $ : the next state $s_{t+1}$ (```next_state```)  in the Q-table with all possible actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the update function of the table Q\n",
    "\n",
    "def update_Q(Q: np.ndarray, state: int, next_state: int, action: int, alpha: float, r: float, gamma: float) -> np.ndarray:\n",
    "    new_Q = Q.copy()\n",
    "    \n",
    "    ### CODE 04 ###\n",
    "    ### BEGIN : # Q-learning update rule\n",
    "    new_Q[state, action] = (1 - alpha) * Q[state, action] + alpha * (r + gamma * np.max(Q[next_state, :])) \n",
    "    \n",
    "    new_Q[state,action] = None\n",
    "    \n",
    "    ### END\n",
    "    \n",
    "    return new_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# array([[0.1 , 0.2 , 0.3 ],\n",
    "#        [1.  , 0.5 , 1.58],\n",
    "#        [0.5 , 1.  , 0.8 ],\n",
    "#        [0.2 , 0.8 , 0.9 ],\n",
    "#        [0.2 , 1.  , 0.3 ]])\n",
    "#---------------------------------------------------------------------\n",
    "Q_t = np.array([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [1.0, 0.5, 0.7],\n",
    "    [0.5, 1.0, 0.8],\n",
    "    [0.2, 0.8, 0.9],\n",
    "    [0.2, 1.0, 0.3]\n",
    "])\n",
    "\n",
    "update_Q(Q_t, state=1, next_state=2, action=2, alpha=0.2, r=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.4. La classe Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, nb_states: int, nb_actions: int, alpha: float, epsilon=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = create_Q(nb_states, nb_actions)\n",
    "    \n",
    "    def set_state(self, state: int):\n",
    "        self.state = state\n",
    "        self.action = 0\n",
    "        \n",
    "    def choose_action(self):\n",
    "        self.action = choose_action(self.Q, self.state, self.epsilon)\n",
    "        return self.action\n",
    "    \n",
    "    def apply(self, next_state: int, action: int, r: float, gamma: float):\n",
    "        self.Q = update_Q(self.Q, self.state, next_state, self.action, self.alpha, r, gamma)\n",
    "        self.state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# array([[-2.  ,  4.  ,  0.  ],\n",
    "#        [-0.36, -0.2 , -2.  ],\n",
    "#        [ 0.  ,  0.  ,  0.  ],\n",
    "#        [-0.2 ,  0.4 ,  0.  ],\n",
    "#        [ 2.  ,  0.  ,  0.  ]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "next_stats_rs = [(0, -1), \n",
    "                 (1, -10), \n",
    "                 (3, -1), \n",
    "                 (1, 2), \n",
    "                 (4, -1), \n",
    "                 (1, 10), \n",
    "                 (1, -10), \n",
    "                 (0, -1), \n",
    "                 (0, 20)]\n",
    "\n",
    "# exploitation: to make it deterministic\n",
    "agent = Agent(nb_states=5, nb_actions=3, alpha=0.2, epsilon=0.) \n",
    "\n",
    "# initial state = 3\n",
    "agent.set_state(state=3)\n",
    "\n",
    "for next_state, r in next_stats_rs:\n",
    "    action = agent.choose_action()\n",
    "    # Environment FeedBack (next_state, r)\n",
    "    agent.apply(next_state, action, r, gamma=0.5)\n",
    "    \n",
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Environment\n",
    "\n",
    "- Here we will implement the cab and passenger problem: (https://arxiv.org/pdf/cs/9905014.pdf).\n",
    "- Our environment is a space divided into $nb_l$ rows and $nb_c$ columns to indicate the position of the cab.\n",
    "- It also contains a number of stops $nb_a$.\n",
    "- The position of the cab is encoded using the row and column number (the coordinates).\n",
    "- The destination is the stop number ($0<= dst < nb_a$).\n",
    "- The position of the passenger is represented by The stop number ```psg``` ($0 <= psg < nb_a$) :\n",
    "    - We set $psg = nb_a$ to indicate that the passenger is inside the cab.\n",
    "\n",
    "#### I.2.1. Encoding and decoding of the states\n",
    "\n",
    "Here we have two functions: \n",
    "- One that encodes the state based on :\n",
    "    1. ```pos```$(x,y)$ : The position of the cab.\n",
    "    2. The number of the startup stop.\n",
    "    3. ```dst``` The number of the destination stop.\n",
    "    4. The number of columns ```nb_c```, rows ```nb_l``` and stops ```nb_a```.\n",
    "\n",
    "with the following equation:\n",
    "    \n",
    "$$ (x * nb_c + y) * (nb_a + 1) * nb_a + (psg * nb_a + dst) $$\n",
    "\n",
    "- The other function decodes a state of position of the cab: \n",
    "    1. Line \n",
    "    2. Column \n",
    "    3. Passenger stop \n",
    "    4. The destination stop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_state(pos: Tuple[int, int], psg: int, dst: int, nb_l: int, nb_c: int, nb_a: int) -> int:\n",
    "    \n",
    "    x = pos[0]\n",
    "    y = pos[1]\n",
    "    \n",
    "    ### CODE 05 ###\n",
    "    ### BEGIN : encoding = x + y * nb_c + psg * nb_l * nb_c + dst * nb_l * nb_c * nb_a  \n",
    "    \n",
    "    encoding = None\n",
    "    \n",
    "    ### END\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# 153\n",
    "#---------------------------------------------------------------------\n",
    "encoder_state((1, 2), 3, 1, 5, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_state(state: int, nb_l:int, nb_c: int, nb_a: int) -> Tuple[int, int, int, int]:\n",
    "    # max number of passenger positions * stop per box\n",
    "    nb_pa = (nb_a + 1) * nb_a \n",
    "    \n",
    "    # passenger position * stop position\n",
    "    pa  = state % nb_pa \n",
    "    dst = pa % nb_a\n",
    "    psg = pa // nb_a\n",
    "    \n",
    "    # Row * Col\n",
    "    lc = state // nb_pa \n",
    "    l  = lc // nb_c\n",
    "    c  = lc % nb_c\n",
    "    \n",
    "    return l, c, psg, dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# (1, 2, 3, 1)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "decoder_state(153, 5, 5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2. Calculer la récompense\n",
    "\n",
    "The reward function has as input: \n",
    "\n",
    "- ```state``` : the current state of the agent.\n",
    "- ```action``` : number of the action chosen by the agent \n",
    "- ```nb_l``` , ```nb_c``` : number of rows and columns in the environment\n",
    "- ```stops``` : a list of stop positions:\n",
    "    - The position is encoded as a tuple (x, y). \n",
    "    - P.S. The tuples are hashable; therefore we can check their existence in the list by using the ```in``` operator.\n",
    "- ```bar``` : a dictionary {pos: list integers} : If a position exists, we will have a list of disallowed  actions (positioning actions).\n",
    "\n",
    "The function must return: \n",
    "    A. the reward\n",
    "    B. the next state \n",
    "    C. And a boolean that indicates the end of the process.\n",
    "\n",
    "**A. The reward:**\n",
    "\n",
    "- For each action performed, a reward of -1 is given, to force the agent to go ahead and make the shortest possible path.\n",
    "- If the agent attempts to drop off or pick up a passenger illegally, a reward of -10 is awarded in addition.\n",
    "    - The \"drop off\" action: is considered illegal (if the passenger is not in the cab) **OR** (if the drop off location is not the destination stop).\n",
    "    - The action \"pick up\": is considered illegal (if the passenger is not in the current position) **OR** (he is already in the car).\n",
    "- If the agent drops off the passenger at the destination stop, it will get an additional +20 reward.\n",
    "\n",
    "**B. The next state:**\n",
    "The next state is the current state except in the following cases:\n",
    "- If a passenger is picked up successfully, the passenger's index will be ```nb_a``` (in the car). \n",
    "- If a passenger is successfully dropped off, the passenger index will be ```dst```. The end will be ```True```.\n",
    "- In the case of a positional action ({0, 1, 2, 3}), if the position is not in the list of barrier ```bar``` or it exists but the action does not exist in the list of forbidden actions, the new position and the new state are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the action and the position, return the new position.\n",
    "# This function does not take into account the constraints\n",
    "\n",
    "def move(pos: Tuple[int, int], action: int) -> Tuple[int, int]:  \n",
    "    # action = 0 (Move to the left)\n",
    "    # action = 1 (Move to the right) \n",
    "    # action = 2 (Move to the)\n",
    "    # action = 3 (Move backwards)\n",
    "    # action = 4 (Pick up a passenger)\n",
    "    # action = 5 (Drop off a passenger)\n",
    "    \n",
    "    if action == 0: \n",
    "        return pos[0], pos[1]  - 1\n",
    "    if action == 1: \n",
    "        return pos[0], pos[1]  + 1\n",
    "    if action == 2: \n",
    "        return pos[0] + 1, pos[1]\n",
    "    if action == 3: \n",
    "        return pos[0] - 1, pos[1]\n",
    "    \n",
    "    # in case the \"action > 3\"\n",
    "    \n",
    "    return pos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# ((2, 2), (1, 2), (1, 1))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "move((1, 2), 2), move((1, 2), 4), move((1, 2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the function that calculates the reward\n",
    "\n",
    "def calculate_reward(state: int, action: int, \n",
    "                        nb_l:int, nb_c: int, \n",
    "                        stops: List[Tuple[int, int]],\n",
    "                        bar: Dict[Tuple[int, int], Set[int]]) -> Tuple[float, int, bool]:\n",
    "    \n",
    "    # number of stops\n",
    "    nb_a = len(stops)\n",
    "    \n",
    "    # Decode State \n",
    "    l, c, psg, dst = decoder_state(state, nb_l, nb_c, nb_a)\n",
    "    # Taxi Position \n",
    "    pos = (l, c)\n",
    "    # Passenger destination position\n",
    "    destination_pos = stops[dst] \n",
    "    \n",
    "    # The reward:\n",
    "    ##  Always apply this reward\n",
    "    reward = -1\n",
    "    next_state = state\n",
    "    fin = False\n",
    "    \n",
    "    # Hint : translate the reward and changing stat rules into if else statements \n",
    "    \n",
    "    ### CODE 06 ###\n",
    "    ### BEGIN : # if action is in {0,1,2,3} : positional action\n",
    "    if action in {0, 1, 2, 3}:\n",
    "        if pos not in bar:\n",
    "            pos = move(pos, action, nb_l, nb_c)\n",
    "            next_state = encoder_state(pos, psg, dst, nb_l, nb_c, nb_a)\n",
    "\n",
    "    # Check if the passenger reaches the destination\n",
    "    if psg == dst:\n",
    "        fin = True\n",
    "        reward = 20\n",
    "\n",
    "    # Check if the taxi picks up the passenger\n",
    "    if pos == stops[psg]:\n",
    "        reward = 10\n",
    "\n",
    "    # Check if the passenger needs to be dropped off\n",
    "    if pos == destination_pos and psg == dst:\n",
    "        reward = 20\n",
    "\n",
    "    # Handle illegal drop-off attempts\n",
    "    if pos in bar and psg == dst:\n",
    "        reward = -10\n",
    "     \n",
    "        # If the agent successfully drops off the passenger at the destination stop\n",
    "        # he will get a +20 reward\n",
    "        # the passenger's index will be dst. \n",
    "        # The end will be True.\n",
    "        elif None :\n",
    "            reward += None\n",
    "            psg = None\n",
    "            next_state = encoder_state(None)\n",
    "            fin = None\n",
    "            \n",
    "    ### End       \n",
    "\n",
    "\n",
    "    return reward, next_state, fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_l, nb_c = 5, 5\n",
    "stops = [(0,0), (0,4), (4,0), (4,3)]\n",
    "nb_a = len(stops)\n",
    "barriers = {\n",
    "    (0, 1): set([1]), # barrier on the right\n",
    "    (0, 2): set([0]), # barrier on the left\n",
    "    \n",
    "    (3, 0): set([1]), # barrier on the right\n",
    "    (3, 1): set([0]), # barrier on the left\n",
    "    \n",
    "    (4, 0): set([1]), # barrier on the right\n",
    "    (4, 1): set([0]), # barrier on the left\n",
    "    \n",
    "    (3, 2): set([1]), # barrier on the right\n",
    "    (3, 3): set([0]), # barrier on the left\n",
    "    \n",
    "    (4, 2): set([1]), # barrier on the right\n",
    "    (4, 3): set([0]), # barrier on the left\n",
    "}\n",
    "\n",
    "tests = [\n",
    "    # (state, action)\n",
    "    # action = 4 (Pick up a passenger)\n",
    "    (encoder_state(pos=(0, 2), psg=4, dst=0, nb_l=nb_l, nb_c=nb_c, nb_a=nb_a), 4), ## pos=(0,2); psg=in the cab; dst=stop0(0, 0)\n",
    "    (encoder_state(pos=(0, 2), psg=1, dst=0, nb_l=nb_l, nb_c=nb_c, nb_a=nb_a), 4), ## pos=(0,2); psg=stop1; dst=stop0(0, 0)\n",
    "    (encoder_state(pos=(0, 4), psg=1, dst=0, nb_l=nb_l, nb_c=nb_c, nb_a=nb_a), 4), ## pos=stop1(0,4); psg=stop1(0,4); dst=stop0(0, 0)\n",
    "    \n",
    "    # action = 5 (Drop off a passenger)\n",
    "    (encoder_state((0, 0), 4, 0, nb_l, nb_c, nb_a), 5), # pos=stop0; psg=in the cab; dst=stop0(0, 0)\n",
    "    (encoder_state((0, 0), 1, 0, nb_l, nb_c, nb_a), 5), # pos=stop0; psg=stop1; dst=stop0(0, 0)\n",
    "    (encoder_state((0, 2), 4, 0, nb_l, nb_c, nb_a), 5), # pos=(0, 2); psg=in the cab e; dst=stop0(0, 0)\n",
    "    (encoder_state((0, 2), 1, 0, nb_l, nb_c, nb_a), 5), # pos=(0, 2); psg=stop1; dst=stop0(0, 0)\n",
    "    \n",
    "    # action = 0 (Move to the left)\n",
    "    (encoder_state((0, 2), 1, 0, nb_l, nb_c, nb_a), 0), # there is a barrier on the left\n",
    "    (encoder_state((3, 0), 1, 0, nb_l, nb_c, nb_a), 0), # there is a barrier on the right\n",
    "    (encoder_state((2, 2), 1, 0, nb_l, nb_c, nb_a), 0), # there is no barrier\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# [(-11, 56, False),\n",
    "#  (-11, 44, False),\n",
    "#  (-1, 96, False),\n",
    "#  (19, 0, True),\n",
    "#  (-11, 4, False),\n",
    "#  (-11, 56, False),\n",
    "#  (-11, 44, False),\n",
    "#  (-1, 44, False),\n",
    "#  (-1, 284, False),\n",
    "#  (-1, 224, False)]\n",
    "#---------------------------------------------------------------------\n",
    "results = []\n",
    "\n",
    "for state, action in tests:\n",
    "    results.append(calculate_reward(state, action, nb_l, nb_c, stops, barriers))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.3. The Environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from IPython.display import HTML, display, clear_output\n",
    "\n",
    "class TaxiEnv():\n",
    "    def __init__(self, nb_l:int, nb_c: int, \n",
    "                 stops: List[Tuple[int, int]], \n",
    "                 bar: Dict[Tuple[int, int], Set[int]], gamma: float = 0.5):\n",
    "        self.actions = ['Left', 'right', 'moving forward', 'backwards', 'Pick-up', 'drop-off']\n",
    "        self.stops = stops\n",
    "        self.nb_l = nb_l\n",
    "        self.nb_c = nb_c\n",
    "        self.nb_states = nb_l * nb_c * (len(stops) + 1) * len(stops)\n",
    "        self.bar = bar\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        for i in range(nb_l):\n",
    "            pos = (i, 0)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "                \n",
    "            # we can't go left\n",
    "            bar[pos].add(0) \n",
    "            \n",
    "            pos = (i, nb_c-1)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "                \n",
    "            # we can't go right\n",
    "            bar[pos].add(1) \n",
    "            \n",
    "        for j in range(nb_c):\n",
    "            pos = (0, j)\n",
    "            \n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            # we can't go forward\n",
    "            bar[pos].add(3) \n",
    "            \n",
    "            pos = (nb_l-1, j)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            # we can't go backwards\n",
    "            bar[pos].add(2) \n",
    "            \n",
    "        \n",
    "    def add_agent(self, alpha: float, epsilon=0.2):\n",
    "        self.agent = Agent(self.nb_states, len(self.actions), alpha, epsilon=epsilon)\n",
    "    \n",
    "    def encoder_state(self, pos: Tuple[int, int], psg: int, dst: int):\n",
    "        return encoder_state(pos, psg, dst, self.nb_l, self.nb_c, len(self.stops))\n",
    "    \n",
    "    def decoder_state(self, state: int) -> Tuple[int, int, int]:\n",
    "        return decoder_state(state, self.nb_l, self.nb_c, len(self.stops))\n",
    "    \n",
    "    def initialize(self, pos: Tuple[int, int], psg: int, dst: int):\n",
    "        state = self.encoder_state(pos, psg, dst)\n",
    "        self.agent.set_state(state)\n",
    "    \n",
    "    def transporter(self, plot=False):\n",
    "        nb_l = self.nb_l\n",
    "        nb_c = self.nb_c\n",
    "        stops = self.stops\n",
    "        bar = self.bar\n",
    "        nb_a = len(stops)\n",
    "        actions = self.actions\n",
    "        state = self.agent.state\n",
    "        \n",
    "        etapes = []\n",
    "        fin = False\n",
    "        rt = 0\n",
    "        \n",
    "        while not fin:\n",
    "            action = self.agent.choose_action()\n",
    "            r, next_state, fin = calculate_reward(state, action, nb_l, nb_c, stops, bar)\n",
    "            etapes.append((self.decoder_state(state), actions[action], r, fin))\n",
    "            self.agent.apply(next_state, action, r, self.gamma)\n",
    "            if plot:\n",
    "                rt += r\n",
    "                html = self.draw()\n",
    "                html += '<div class=\"cont\">'\n",
    "                html += f'<p>Step: {len(etapes)}</p>'\n",
    "                html += f'<p>State: {state}</p>'\n",
    "                html += f'<p>Action: {actions[action]}</p>'\n",
    "                html += f'<p>Reward: {r}</p>'\n",
    "                html += f'<p>Sum of Rewards: {rt}</p>'\n",
    "                html += '</div>'\n",
    "                time.sleep(0.5)\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(html))\n",
    "                sys.stdout.flush()\n",
    "            state = next_state\n",
    " \n",
    "        return etapes\n",
    "    \n",
    "    def draw(self):\n",
    "        bordures = ['l', 'r', 'b', 't']\n",
    "        \n",
    "        nb_a = len(self.stops)\n",
    "        \n",
    "        if hasattr(self.agent, 'state'):\n",
    "            l, c, psg, dst = decoder_state(self.agent.state, self.nb_l, self.nb_c, nb_a)\n",
    "        else:\n",
    "            l, c, psg, dst = None, None, None, None\n",
    "        \n",
    "        html = \"\"\"<style>\n",
    "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
    "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
    "                                                background: white; padding: 1px;}\n",
    "                \n",
    "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
    "                table#t tr td.l {border-left: 2px solid red ;}\n",
    "                table#t tr td.r {border-right: 2px solid red ;}\n",
    "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
    "                table#t tr td.t {border-top: 2px solid red ;}\n",
    "                table#t tr td.stop {background: yellow;}\n",
    "                </style>\n",
    "                <div class=\"cont\">\n",
    "                <table id=\"t\">\n",
    "                \"\"\"\n",
    "        for i in range(self.nb_l):\n",
    "            html += \"<tr>\"\n",
    "            for j in range(self.nb_c):\n",
    "                cls = None\n",
    "                html += \"<td \"\n",
    "                if (i, j) in self.bar:\n",
    "                    cls = 'class=\"'\n",
    "                    bl = self.bar[(i, j)]\n",
    "                    for b in bl:\n",
    "                        cls += bordures[b] + ' '\n",
    "                if (i, j) in self.stops:\n",
    "                    if not cls:\n",
    "                        cls = 'class=\"'\n",
    "                    cls += 'stop'\n",
    "                if cls:\n",
    "                    cls += '\"'\n",
    "                    html += cls\n",
    "                html += '>'\n",
    "                cont = ''\n",
    "                if dst != None and self.stops[dst] == (i, j):\n",
    "                    cont = '🏲'\n",
    "                if psg != None and psg != nb_a and self.stops[psg] == (i, j):\n",
    "                    cont += '👽'\n",
    "                if (l, c) == (i, j):\n",
    "                    if psg != None and psg != nb_a:\n",
    "                        cont += '🚖'\n",
    "                    else:\n",
    "                        cont += '🚍'\n",
    "                if not cont:\n",
    "                    cont = ':'\n",
    "                html += cont + '</td>'\n",
    "            html += '</tr>'\n",
    "            \n",
    "        html += '</table></div>'\n",
    "        \n",
    "        return html\n",
    "        \n",
    "print('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stops = [(0,0), (0,4), (4,0), (4,3)]\n",
    "barriers = {\n",
    "    (0, 1): set([1]), # barrier on the right\n",
    "    (0, 2): set([0]), # barrier on the left\n",
    "    (3, 0): set([1]), # barrier on the right\n",
    "    (4, 0): set([1]), # barrier on the right\n",
    "    (3, 1): set([0]), # barrier on the left\n",
    "    (4, 1): set([0]), # barrier on the left\n",
    "    (3, 2): set([1]), # barrier on the right\n",
    "    (4, 2): set([1]), # barrier on the right\n",
    "    (3, 3): set([0]), # barrier on the left\n",
    "    (4, 3): set([0]), # barrier on the left\n",
    "}\n",
    "\n",
    "taxi = TaxiEnv(nb_l=5,nb_c=5, stops=stops, bar=barriers)\n",
    "\n",
    "        \n",
    "taxi.add_agent(alpha=0.1, epsilon=0.1)\n",
    "taxi.initialize(pos=(3, 1), psg=2, dst=0)\n",
    "\n",
    "\n",
    "html = taxi.draw()\n",
    "display(HTML(html))\n",
    "hist = taxi.transporter(plot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test after running the same initialization several times\n",
    "\n",
    "for i in range(1000):\n",
    "    taxi.initialize((3, 1), 2, 0)\n",
    "    taxi.transporter() \n",
    "\n",
    "taxi.initialize((3, 1), 2, 0)\n",
    "hist = taxi.transporter(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to print the history of the steps\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the learning with random initializations\n",
    "\n",
    "stops2 = [(0,0), (0,4), (4,0), (4,3)]\n",
    "barriers2 = {\n",
    "    (0, 1): set([1]), # right-hand barrier\n",
    "    (0, 2): set([0]), # left-hand barrier\n",
    "    (3, 0): set([1]), # right-hand barrier\n",
    "    (4, 0): set([1]), # right-hand barrier\n",
    "    (3, 1): set([0]), # left-hand barrier\n",
    "    (4, 1): set([0]), # left-hand barrier\n",
    "    (3, 2): set([1]), # right-hand barrier\n",
    "    (4, 2): set([1]), # right-hand barrier\n",
    "    (3, 3): set([0]), # left-hand barrier\n",
    "    (4, 3): set([0]), # left-hand barrier\n",
    "}\n",
    "\n",
    "taxi2 = TaxiEnv(5, 5, stops2, barriers2)\n",
    "taxi2.add_agent(0.1, 0.1)\n",
    "\n",
    "def random_exec(taxi_env, plot=False):\n",
    "    pos = np.random.randint(5), np.random.randint(5)\n",
    "    psg, dst = np.random.randint(len(stops2)), np.random.randint(len(stops2))\n",
    "    taxi_env.initialize(pos, psg, dst)\n",
    "    return taxi_env.transporter(plot=plot) \n",
    "\n",
    "for i in range(10000):\n",
    "    random_exec(taxi2, plot=False)\n",
    "    \n",
    "print('End')\n",
    "\n",
    "hist = random_exec(taxi2, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application and analysis\n",
    "\n",
    "Here are some tools for testing reinforcement learning:\n",
    "\n",
    "- OpenAI Baselines: https://github.com/openai/baselines\n",
    "- Intel Coach: https://github.com/IntelLabs/coach\n",
    "- Stable Baselines: https://github.com/DLR-RM/stable-baselines3\n",
    "- TF-Agents: https://github.com/tensorflow/agents\n",
    "- Keras-RL: https://github.com/keras-rl/keras-rl\n",
    "- Tensorforce: https://github.com/tensorforce/tensorforce\n",
    "- Chainer RL: https://github.com/chainer/chainerrl\n",
    "- Mushroom RL: https://github.com/MushroomRL/mushroom-rl\n",
    "- Acme: https://github.com/deepmind/acme\n",
    "- Dopamine: https://github.com/google/dopamine\n",
    "- RAY: https://github.com/ray-project/ray\n",
    "\n",
    "Environments :\n",
    "\n",
    "- Gym: https://gym.openai.com/\n",
    "- iGibson: http://svl.stanford.edu/igibson/\n",
    "\n",
    "We will use \"MushroomRL\" since the tool implements the classical methods.\n",
    "Also, we will use [\"Gym\"](https://gym.openai.com/) to generate the environments. \n",
    "The Taxi environment will be used since it doesn't consume a lot of resources (memory and computation).\n",
    "You can consult \"Gym\" for more complex environments like games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# package installation\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install gym\n",
    "!{sys.executable} -m pip install mushroom_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Exploration vs. Exploitation\n",
    "\n",
    "Here, we want to test the effect of exploration/exploitation trade-off. \n",
    "To do this, we will test with different values of epsilon :\n",
    "\n",
    "- 0.0 : exploitation (100%).\n",
    "- 0.5 : exploitation (50%) and exploration (50%).\n",
    "- 0.9 : exploration (90%) and exploitation (10%).\n",
    "\n",
    "The default maximum number of steps is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.core import Environment\n",
    "from mushroom_rl.policy import EpsGreedy\n",
    "from mushroom_rl.algorithms.value import QLearning\n",
    "from mushroom_rl.utils.dataset import compute_J\n",
    "from mushroom_rl.utils.parameters import Parameter\n",
    "from mushroom_rl.core import Core\n",
    "from mushroom_rl.utils.callbacks import CollectDataset\n",
    "from mushroom_rl.utils.callbacks.callback import Callback\n",
    "from mushroom_rl.utils.dataset import parse_dataset, episodes_length\n",
    "\n",
    "# number of executions\n",
    "NBR_EPISODES = 1000 \n",
    "\n",
    "env = Environment.make('Gym', 'Taxi-v3')\n",
    "\n",
    "epsilons = [.0, .5, .9]\n",
    "\n",
    "tests = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    epsilon = Parameter(value=eps)\n",
    "    pi = EpsGreedy(epsilon=epsilon)\n",
    "    agent = QLearning(env.info, pi, learning_rate=Parameter(value=.3))\n",
    "    \n",
    "    collect_dataset = CollectDataset()\n",
    "    callbacks = [collect_dataset]\n",
    "    \n",
    "    core = Core(agent, env, callbacks_fit=callbacks)\n",
    "    core.learn(n_episodes=NBR_EPISODES, n_steps_per_fit=1)\n",
    "    \n",
    "    res = {}\n",
    "    res['nbr_steps'] = episodes_length(collect_dataset.get())\n",
    "    tests.append(res)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, test in enumerate(tests):\n",
    "    plt.plot(test['nbr_steps'], label='epsilon=' + str(epsilons[i]))\n",
    "plt.xlabel('Episodes') \n",
    "plt.ylabel('Number of steps') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\[Q1\\]: Analyze the results:**\n",
    "\n",
    "1. Why can't the algorithm with more exploration minimize the number of steps after several sessions (episodes) ?\n",
    "2. Why there are episodes that have a minimal number of steps especially in the last episodes (always in the algorithm with more exploration) ?\n",
    "3. Why does the one with only exploitation take less steps in each episode ?\n",
    "4. In this case, what is the purpose of the exploration ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A1:Begin]\n",
    "\n",
    "**Answer 01**\n",
    "\n",
    "1. The algorithm with more exploration may not minimize the number of steps after several episodes because it continues to explore different actions, even if it has already found a policy that is reasonably good. Exploration introduces randomness in action selection, and the agent might keep trying different actions, preventing it from converging quickly to an optimal policy.\n",
    "\n",
    "2. Episodes with a minimal number of steps in the last episodes, especially in the algorithm with more exploration, could be due to the agent discovering an optimal or near-optimal policy. As the algorithm explores, it gradually refines its understanding of the environment and learns a better policy. In some cases, the exploration might lead to a breakthrough where the agent finds a highly efficient path to reach the goal, resulting in episodes with minimal steps. However, the randomness in exploration can lead to variability in episode lengths.\n",
    "3. The algorithm with only exploitation takes fewer steps in each episode because it exploits the current knowledge and tends to choose actions that it believes are the most optimal based on the learned Q-values. Exploitation-focused policies often converge faster to a locally optimal policy because they consistently choose actions that have shown to be effective in the past. However, they are also more prone to getting stuck in local optima and might miss out on discovering better actions.\n",
    "4. Exploration is essential for the following reasons:\n",
    "\n",
    "    Discovering the Environment: Exploration allows the agent to discover the dynamics and features of the environment. Without exploration, the agent might miss critical states or actions, hindering its ability to learn an optimal policy.\n",
    "\n",
    "    Avoiding Local Optima: Exploration is crucial for preventing the agent from converging too quickly to a suboptimal policy (local optima). By exploring less-frequently chosen actions, the agent has the potential to find a globally optimal policy.\n",
    "\n",
    "    Adaptation to Changes: Exploration helps the agent adapt to changes in the environment. If the environment dynamics change, exploration allows the agent to update its knowledge and adapt its policy accordingly.\n",
    "\n",
    "[A1:End]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Learning rate\n",
    "\n",
    "In this last part, we will investigate the effect of the learning rate on the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of executions\n",
    "NBR_EPISODES = 1000\n",
    "\n",
    "env = Environment.make('Gym', 'Taxi-v3')\n",
    "\n",
    "lrs = [.1, .2, .3]\n",
    "\n",
    "tests = []\n",
    "\n",
    "for lr in lrs:\n",
    "    epsilon = Parameter(value=0.1)\n",
    "    pi = EpsGreedy(epsilon=epsilon)\n",
    "    agent = QLearning(env.info, pi, learning_rate=Parameter(value=lr))\n",
    "    \n",
    "    collect_dataset = CollectDataset()\n",
    "    callbacks = [collect_dataset]\n",
    "    \n",
    "    core = Core(agent, env, callbacks_fit=callbacks)\n",
    "    core.learn(n_episodes=NBR_EPISODES, n_steps_per_fit=1)\n",
    "    \n",
    "    res = {}\n",
    "    res['nbr_steps'] = episodes_length(collect_dataset.get())\n",
    "    tests.append(res)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, test in enumerate(tests):\n",
    "    plt.plot(test['nbr_steps'], label='Learning Rates=' + str(lrs[i]))\n",
    "plt.xlabel('Episodes') \n",
    "plt.ylabel('Number of steps') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\[Q2\\]: Analyze the results:**\n",
    "\n",
    "- What is the effect of $\\alpha$ on the number of steps after each $n$ episodes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A2:Begin]\n",
    "\n",
    "**Answer 02**\n",
    "\n",
    " The learning rate ($\\alpha$) in the Q-learning algorithm plays a crucial role in determining how much the Q-values are updated during each learning iteration. The effect of the learning rate on the number of steps after each \\(n\\) episodes can be analyzed as follows:\n",
    "\n",
    "1. Low Learning Rate:\n",
    "   When the learning rate is low, the Q-values are updated very slowly. The agent is conservative in updating its knowledge based on new experiences. This can lead to slow convergence, and the agent may require a large number of episodes to learn an optimal policy. However, the learned policy may be more stable and less sensitive to fluctuations in the environment.\n",
    "\n",
    "2. Medium Learning Rate:\n",
    "   A moderate learning rate allows the agent to update its Q-values at a reasonable pace. This often results in faster convergence compared to a low learning rate. The agent can adapt to changes in the environment more quickly, leading to improvements in the learned policy over time. The number of steps after each \\(n\\) episodes is expected to decrease more rapidly compared to a low learning rate.\n",
    "\n",
    "3. High Learning Rate\n",
    "   A high learning rate results in more rapid updates to the Q-values. While this can lead to fast initial learning, it also makes the agent more sensitive to noise and fluctuations in the environment. In some cases, a high learning rate might cause the agent to oscillate or converge to suboptimal policies due to overreacting to individual experiences. The number of steps after each \\(n\\) episodes may decrease quickly initially but could stabilize or exhibit oscillations later.\n",
    "\n",
    "[A2:End]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\[Q3\\]: Analyze the results:**\n",
    "\n",
    "Looking at this diagram and the one before, we can notice that :\n",
    "- The evolution with $\\epsilon=0$ is almost like the one with $\\alpha=0.3$ \n",
    "- and the evolution with $\\epsilon=0.5$ is almost like the one with $\\alpha=0.1$. \n",
    "\n",
    "In this case, can we say that there is a direct relation between the two parameters (in other words, can we replace one othe them as a function of the other) ? if yes, can you explain that ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A3:Begin]\n",
    "\n",
    "**Answer 03**\n",
    "ϵ=0 and α=0.3α=0.3:\n",
    "\n",
    "    Both scenarios result in a more exploitation-focused strategy. A low exploration parameter (ϵ=0ϵ=0) implies that the agent almost always chooses actions based on its current knowledge, while a moderate learning rate (α=0.3α=0.3) ensures that the agent updates its Q-values at a reasonable pace. This combination can lead to stable, but potentially slower, convergence.\n",
    "\n",
    "ϵ=0.5ϵ=0.5 and α=0.1α=0.1:\n",
    "\n",
    "    Both scenarios involve a higher level of exploration. A moderate exploration parameter (ϵ=0.5ϵ=0.5) means the agent explores actions with a probability of 50%, and a low learning rate (α=0.1α=0.1) suggests that the agent is more conservative in updating its Q-values. This combination can result in slower initial convergence but may offer more stability and robustness to changes in the environment.\n",
    " \n",
    " \n",
    "\n",
    "[A2:End]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
